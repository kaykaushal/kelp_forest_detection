{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os \n",
    "import warnings \n",
    "warnings.filterwarnings('ignore')\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import torch \n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader, random_split\n",
    "import torchvision.transforms as transforms\n",
    "import optuna\n",
    "\n",
    "from pathlib import Path\n",
    "import xarray as xr\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## set working direcitory and import project module\n",
    "HOME_DIR = '/home/jovyan/open_pluto/kelp_forest_detection'\n",
    "os.chdir(HOME_DIR)\n",
    "from src.dataset import KelpDataset\n",
    "from src.model import BaseUnet, UNetPlus, DiceLoss\n",
    "from src.train import Trainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set variables and parameters \n",
    "TRAIN_DIR = os.path.join(HOME_DIR, 'data/training/')\n",
    "# Data prams\n",
    "IMAGE_DIR = os.path.join(TRAIN_DIR,'train_satellite')\n",
    "MASK_DIR = os.path.join(TRAIN_DIR,'train_kelp')\n",
    "\n",
    "DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "# Model params\n",
    "WORKER = 2\n",
    "CHANNEL_IN = 7\n",
    "CHANNEL_OUT = 1\n",
    "BATCH_SIZE = 4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get Data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get metdatafile for training and test split \n",
    "meta_df = pd.read_csv('metadata_kelp.csv')\n",
    "df = meta_df[meta_df['in_train'] == True].head(2000)\n",
    "train_df, valid_df = train_test_split(df, test_size = .2, random_state=42)\n",
    "train_files = train_df['filename'].tolist()\n",
    "valid_files = valid_df['filename'].tolist()\n",
    "# Check length\n",
    "len(train_files), len(valid_files)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.manual_seed(12)\n",
    "transform = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    #transforms.RandomHorizontalFlip(),\n",
    "    #transforms.Resize((256, 256)),\n",
    "    \n",
    "])\n",
    "# Custome datasets\n",
    "# Create a custom training custom dataset for training dataset\n",
    "train_dataset = KelpDataset(image_dir=IMAGE_DIR, mask_dir=MASK_DIR,\n",
    "                            transform=transform, filename_list=train_files)\n",
    "# Create a custom training custom dataset for validation set\n",
    "valid_dataset = KelpDataset(image_dir=IMAGE_DIR, mask_dir=MASK_DIR,\n",
    "                            transform=transform, filename_list=valid_files)\n",
    "# Load dataloader for train and validation set\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True, \n",
    "                              num_workers=0, pin_memory=True,  prefetch_factor=2)\n",
    "# Validation loader\n",
    "valid_dataloader = DataLoader(valid_dataset, batch_size=BATCH_SIZE, shuffle=False, \n",
    "                              num_workers=0, pin_memory=True,  prefetch_factor=WORKER)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "img, mask = next(iter(train_dataloader))\n",
    "img.shape, mask.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(mask[3][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "element_at_index_1_0 = mask[3][0]\n",
    "\n",
    "# Flatten the tensor to a 1D array\n",
    "flattened_values = element_at_index_1_0.flatten()\n",
    "\n",
    "# Plot the histogram\n",
    "plt.hist(flattened_values.numpy(), bins=50, color='blue', alpha=0.7)\n",
    "plt.title('Distribution / Histogram Chart')\n",
    "plt.xlabel('Values')\n",
    "plt.ylabel('Frequency')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.cuda.device_count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the objective function for Optuna\n",
    "def objective(trial):\n",
    "    # Define hyperparameters to be optimized\n",
    "    in_channels = 7  # Change if your input channels are different\n",
    "    out_channels = 1\n",
    "    #features = [trial.suggest_int(f'features_{i}', 32, 512) for i in range(4)]\n",
    "    dropout_prob = trial.suggest_float('dropout_prob', 0.0, 0.5)\n",
    "    # Define model\n",
    "    model = BaseUnet(\n",
    "        in_channels=in_channels,\n",
    "        out_channels=out_channels,\n",
    "        #features=features,\n",
    "        dropout_prob=dropout_prob\n",
    "    )\n",
    "    \n",
    "    # Set training hyperparameter\n",
    "    lr = trial.suggest_float('lr', 1e-5, 1e-2, log=True)\n",
    "    # Choose optimizer and its hyperparameters\n",
    "    optimizer_name = trial.suggest_categorical('optimizer', ['Adam', 'Adamax'])\n",
    "    weight_decay = trial.suggest_float('weight_decay', 1e-6, 1e-2, log=True)\n",
    "    if optimizer_name == 'Adam':\n",
    "        optimizer = torch.optim.Adam(model.parameters(), lr=lr, weight_decay=weight_decay)\n",
    "    else:\n",
    "        optimizer = torch.optim.Adamax(model.parameters(), lr=lr, weight_decay=weight_decay)\n",
    "\n",
    "    # Choose loss function\n",
    "    loss_name = trial.suggest_categorical('loss', ['MSELoss', 'BCELoss', 'HuberLoss'])\n",
    "    if loss_name == 'MSELoss':\n",
    "        criterion = torch.nn.MSELoss()\n",
    "    elif loss_name == 'HuberLoss':\n",
    "        criterion = torch.nn.HuberLoss()\n",
    "    else:\n",
    "        # Assuming you have a custom DiceLoss implementation\n",
    "        criterion = torch.nn.BCEWithLogitsLoss()\n",
    "\n",
    "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "    # Instantiate Trainer with Optuna trial\n",
    "    trainer = Trainer(\n",
    "        model=model.to(device),\n",
    "        train_dataloader=train_dataloader,\n",
    "        valid_dataloader=valid_dataloader,\n",
    "        criterion=criterion,\n",
    "        optimizer=optimizer,\n",
    "        device=device,\n",
    "        tb_path='tensorboard/runs/',\n",
    "        checkpoint_path='tune/',\n",
    "        trial=trial\n",
    "    )\n",
    "    \n",
    "    # Train the model\n",
    "    trainer.train(epochs=10)  # Adjust the number of epochs as needed\n",
    "\n",
    "    # Return a scalar value indicating the performance\n",
    "    return trainer.best_valid_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create an optimization study and perform optimization\n",
    "study = optuna.create_study(direction='minimize')  # Optimize for minimum validation loss\n",
    "study.optimize(objective, n_trials=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  },
  "vscode": {
   "interpreter": {
    "hash": "38b11bb1e1d1abe82825b6b307868aabe11054468bfbc516faff0fc6d7286e5f"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
